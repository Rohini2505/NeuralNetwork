# NeuralNetwork
Vectorized implementation of a general feedforward neural network in Python
- Four activation functions available: sigmoid, softmax, ReLU, and Leaky ReLU
- Three parameter initialization functions
- Three cost functions available: quadratic, cross entropy, log likelihood
- Network is trained with stochastic gradient descent, variable batch size
- L2 weight regularization available
 
Two example applications are provided. These demonstrate the effect of different parameter settings and network architectures on performance
- Example1: Two class classification: Separating a ball and a donut
- Example2: MNIST

