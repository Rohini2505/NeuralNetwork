{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "MIT License\n",
    "\n",
    "Copyright (c) [2016] [Laura Graesser]\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE.\n",
    "\n",
    "This program allows the user to implement a general neural network, \n",
    "with an arbitrary number of layers and nodes per layer\n",
    "It is intended to be an educational tool. No error checking is included so as\n",
    "to simplify the code.\n",
    "\n",
    "It was inspired by reading Michael Nielsen's excellent online book \n",
    "Neural Networks and Deep Learning, available at\n",
    "http://neuralnetworksanddeeplearning.com/ so shares a similar structure to\n",
    "his program\n",
    "\n",
    "The main differences in functionality are that this is a vectorized implementation, \n",
    "this offers four different types of activation functions: Sigmoid, Relu, Leaky Relu, and Softmax,\n",
    "it is possible to specify a different activation function for the hidden layers and output layer,\n",
    "it offers different weight initializers, \n",
    "and operator overloading was used to simplify the feedforward step\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=8, suppress=True)\n",
    "\n",
    "'''The Matrix class simplifies the appearance of matrix multiplication and addition,\n",
    "   and was an excercise in operator overloading.\n",
    "   If w, x and b are matrices of appropriate dimensions and W, X and B are\n",
    "   the corresponding instances of the Matrix class then\n",
    "   np.dot(w, x) + b is simplified to W * X + B\n",
    "\n",
    "   Whilst it simplifies the appearance of the feedforward step, in practice this \n",
    "   did not turn out to be as useful as I had hoped because of the need to transpose\n",
    "   matrices and carry out element wise operations.\n",
    "\n",
    "   A better implementation would overload other operators to simplify the appearance\n",
    "   of the transpose and element wise multiplication operations'''\n",
    "class Matrix:\n",
    "    def __init__(self, matrix):\n",
    "        self.matrix = matrix\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        return Matrix(np.dot(self.matrix, other.matrix))\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        return Matrix(self.matrix + other.matrix)\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return Matrix(self.matrix + other.matrix)\n",
    "\n",
    "'''Cost function comments:\n",
    "   output is a 2D matrix containing the predicted output of a neural network \n",
    "   for a batch of training data\n",
    "   \n",
    "   y is 2D matrix containing the correct output for a batch of training data\n",
    "   \n",
    "   Each row corresponds to a data example, each column to an output node/feaure\n",
    "   \n",
    "   Element wise multiplication instead of a dot product is used to ensure that relevant\n",
    "   values are squared when there are multiple examples and multiple output features'''           \n",
    "    \n",
    "class QuadraticCost:\n",
    "    ''' Cost = 1 / 2n sum_x || y - output ||^ 2\n",
    "        Can be used with any activation function in the output layer, however sigmoid is preferred''' \n",
    "    cost_type = \"QuadraticCost\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_cost(output, y, lmda, weights):\n",
    "        '''Cost function cost'''\n",
    "        num_data_points = output.shape[0]\n",
    "        diff = y - output\n",
    "        \n",
    "        '''Regularization cost'''\n",
    "        sum_weights = 0\n",
    "        for w in weights:\n",
    "            sum_weights += np.sum(np.multiply(w.matrix,w.matrix))\n",
    "        regularization = (lmda * sum_weights) / (num_data_points * 2)\n",
    "        \n",
    "        return  np.sum(np.multiply(diff, diff)) / (2 * num_data_points) + regularization\n",
    "    \n",
    "    @staticmethod\n",
    "    def cost_prime(output, y):\n",
    "        '''Derivative of the quadratic cost function'''\n",
    "        return output - y\n",
    "    \n",
    "class CrossEntropyCost:\n",
    "    ''' Cost = -1 / n sum_x (y * ln(output) + (1 - y)*ln(1- output))\n",
    "        Should be used with a sigmoid output layer'''\n",
    "    cost_type = \"CrossEntropyCost\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_cost(output, y, lmda, weights):\n",
    "        '''Cost function cost'''\n",
    "        num_data_points = output.shape[0]\n",
    "        interim = y * np.log(output) + (1 - y) * np.log(1 - output)\n",
    "        \n",
    "        '''Regularization cost'''\n",
    "        sum_weights = 0\n",
    "        for w in weights:\n",
    "            sum_weights += np.sum(np.multiply(w.matrix,w.matrix))\n",
    "        regularization = (lmda * sum_weights) / (num_data_points * 2)\n",
    "        \n",
    "        return  (-1 / num_data_points) * np.sum(interim) + regularization\n",
    "    \n",
    "    @staticmethod\n",
    "    def cost_prime(output, y):\n",
    "        '''Derivative of the cross entropy cost function\n",
    "           ASSUMES that only sigmoid activation units are used in the output layer\n",
    "           Derivative is not correct for other output layer activation functions, such as the ReLU\n",
    "           Any activation function in the hidden layer can be used'''\n",
    "        return output - y\n",
    "    \n",
    "class LogLikelihoodCost:\n",
    "    ''' Cost = -1 / n ln output_c\n",
    "        output_c is the output of the model for the correct answer, \n",
    "        this can be implemented by y * ln output_c since y will be 0 for the all but the correct answer\n",
    "        Should be used with a softmax output layer'''\n",
    "    cost_type = \"LogLikelihoodCost\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_cost(output, y, lmda, weights):\n",
    "        '''Cost function cost'''\n",
    "        num_data_points = output.shape[0]\n",
    "        interim = y * np.log(output)\n",
    "        \n",
    "        '''Regularization cost'''\n",
    "        sum_weights = 0\n",
    "        for w in weights:\n",
    "            sum_weights += np.sum(np.multiply(w.matrix,w.matrix))\n",
    "        regularization = (lmda * sum_weights) / (num_data_points * 2)\n",
    "        \n",
    "        return  (-1 / num_data_points) * np.sum(interim) + regularization\n",
    "    \n",
    "    @staticmethod\n",
    "    def cost_prime(output, y):\n",
    "        '''Derivative of the log likelihood cost function\n",
    "           ASSUMES that only softmax activation units are used in the output layer\n",
    "           Derivative is not correct for other output layer activation functions, such as the ReLU\n",
    "           Any activation function in the hidden layer can be used'''\n",
    "        return output - y    \n",
    "    \n",
    "class SigmoidActivation:\n",
    "    @staticmethod\n",
    "    def fn(x):\n",
    "        '''Assumes x is an instance of the Matrix class'''\n",
    "        return 1 / (1 + np.exp(-x.matrix))\n",
    "    \n",
    "    @staticmethod\n",
    "    def prime(x):\n",
    "        '''Assumes x is an instance of the Matrix class\n",
    "           Derivative of the sigmoid function'''\n",
    "        return np.multiply(SigmoidActivation.fn(x), (1 - SigmoidActivation.fn(x))) \n",
    "    \n",
    "class ReluActivation:\n",
    "    '''Should not be used in the output layer, only hidden layers'''\n",
    "    @staticmethod\n",
    "    def fn(x):\n",
    "        '''Assumes x is an instance of the Matrix class'''\n",
    "        y = np.copy(x.matrix)\n",
    "        y = Matrix(y)\n",
    "        ltzero_indices = y.matrix<0\n",
    "        y.matrix[ltzero_indices] = 0\n",
    "        return y.matrix\n",
    "    \n",
    "    @staticmethod\n",
    "    def prime(x):\n",
    "        '''Assumes x is an instance of the Matrix class'''\n",
    "        ''' Derivative of the RELU function'''\n",
    "        y = np.copy(x.matrix)\n",
    "        y = Matrix(y)\n",
    "        ltzero_indices = y.matrix<0\n",
    "        other_indices = y.matrix>=0\n",
    "        y.matrix[ltzero_indices] = 0\n",
    "        y.matrix[other_indices] = 1\n",
    "        return y.matrix\n",
    "\n",
    "class LeakyReluActivation:\n",
    "    '''Should not be used in the output layer, only hidden layers'''\n",
    "    @staticmethod\n",
    "    def fn(x):\n",
    "        '''Assumes x is an instance of the Matrix class'''\n",
    "        y = np.copy(x.matrix)\n",
    "        y = Matrix(y)\n",
    "        ltzero_indices = y.matrix<0\n",
    "        y.matrix[ltzero_indices] = y.matrix[ltzero_indices] * 0.1\n",
    "        return y.matrix\n",
    "    \n",
    "    @staticmethod\n",
    "    def prime(x):\n",
    "        '''Assumes x is an instance of the Matrix class'''\n",
    "        ''' Derivative of the LRELU function'''\n",
    "        y = np.copy(x.matrix)\n",
    "        y = Matrix(y)\n",
    "        ltzero_indices = y.matrix<0\n",
    "        other_indices = y.matrix>=0\n",
    "        y.matrix[ltzero_indices] = 0.1\n",
    "        y.matrix[other_indices] = 1\n",
    "        return y.matrix    \n",
    "    \n",
    "class SoftmaxActivation:\n",
    "    @staticmethod\n",
    "    def fn(x):\n",
    "        '''Assumes x is an instance of the Matrix class'''\n",
    "        '''Subtracting large constant from each of x values to prevent overflow'''\n",
    "        y = np.copy(x.matrix)\n",
    "        y = Matrix(y)\n",
    "        max_per_row = np.amax(y.matrix, axis=1)\n",
    "        max_per_row = max_per_row.reshape((max_per_row.shape[0], 1))\n",
    "        y.matrix = y.matrix - max_per_row\n",
    "        '''Adding small constant to prevent underflow'''\n",
    "        exp_y = np.exp(y.matrix) + 0.001\n",
    "        exp_y_sum = np.sum(exp_y, axis=1)\n",
    "        exp_y_sum = np.reshape(exp_y_sum,(exp_y_sum.shape[0],1))\n",
    "        return exp_y / exp_y_sum\n",
    "    \n",
    "    @staticmethod\n",
    "    def prime(x):\n",
    "        '''Assumes x is an instance of the Matrix class\n",
    "           Derivative of the softmax function'''\n",
    "        sftmax = SoftmaxActivation.fn(x) \n",
    "        return np.multiply(sftmax, (1 - sftmax))\n",
    "\n",
    "class NeuralNet:\n",
    "    def __init__(self, size, costfn=QuadraticCost, activationfnHidden=SigmoidActivation, \\\n",
    "                activationfnLast=SigmoidActivation):\n",
    "        '''size = list of integers specifying the number of nodes per layer. Includes input and output layers. \n",
    "           e.g.(100,50,10) is a three layers network with one input layer, one hidden layer and one output layer\n",
    "           costfn = cost function for the network. Should be an instance of one of the cost function classes\n",
    "           activationfnHidden = activation function for all of the hidden nodes. Should be an instance\n",
    "           of one of the activation function classes\n",
    "           activationfnLast = activation function for the nodes in the last (output) layer. Should be an \n",
    "           instance of one of the activation function classes'''\n",
    "        self.weights = []\n",
    "        for a, b in zip(size[:-1], size[1:]):\n",
    "            self.weights.append(np.zeros((a,b)))\n",
    "        self.biases = []\n",
    "        for b in size[1:]:\n",
    "            self.biases.append(np.zeros((1, b)))\n",
    "        self.layers = len(size)\n",
    "        self.costfn = costfn\n",
    "        self.activationfnHidden = activationfnHidden\n",
    "        self.activationfnLast = activationfnLast\n",
    "        \n",
    "    def initialize_variables(self):\n",
    "        np.random.seed(1)\n",
    "        i = 0\n",
    "        for w in self.weights:\n",
    "            self.weights[i] = Matrix((np.random.uniform(-1, 1, size=w.shape) / np.sqrt(w.shape[0])))\n",
    "            i += 1\n",
    "        i = 0\n",
    "        for b in self.biases:\n",
    "            self.biases[i] = Matrix(np.random.uniform(-1, 1, size=b.shape))\n",
    "            i += 1\n",
    "            \n",
    "    def initialize_variables_normalized(self):\n",
    "        '''Normalized initialization proposed by Glorot and Bengio, 2010\n",
    "           Suggested for deep networks. Does not appear to be better than\n",
    "           initialize_variables() for networks with 3 layers'''\n",
    "        np.random.seed(1)\n",
    "        i = 0\n",
    "        for w in self.weights:\n",
    "            self.weights[i] = Matrix(((np.random.uniform(-1, 1, size=w.shape) * np.sqrt(6))\\\n",
    "                                      / np.sqrt(w.shape[0] + w.shape[1])))\n",
    "            i += 1\n",
    "        i = 0\n",
    "        for b in self.biases:\n",
    "            self.biases[i] = Matrix(np.random.uniform(-1, 1, size=b.shape))\n",
    "            i += 1\n",
    "            \n",
    "    def initialize_variables_alt(self):\n",
    "        '''Appears to be effective for shallow networks (~3 layers) with cross-entropy cost + ReLU hidden layers'''\n",
    "        np.random.seed(1)\n",
    "        i = 0\n",
    "        for w in self.weights:\n",
    "            self.weights[i] = Matrix((np.random.normal(size=w.shape) / w.shape[1]))\n",
    "            i += 1\n",
    "        i = 0\n",
    "        for b in self.biases:\n",
    "            self.biases[i] = Matrix(np.random.normal(size=b.shape))\n",
    "            i += 1\n",
    "\n",
    "    def feedforward(self, data):\n",
    "        '''Data = batch of input data, 2D matrix, examples x features\n",
    "           Assumes data is structured as an m x n numpy array, examples x features\n",
    "           Returns neural network output for this batch of data'''\n",
    "        z = Matrix(data)\n",
    "        for w, b in zip(self.weights[0:-1], self.biases[0:-1]):\n",
    "            z = Matrix(self.activationfnHidden.fn(z * w + b))\n",
    "        z = Matrix(self.activationfnLast.fn(z * self.weights[-1] + self.biases[-1]))\n",
    "        return z.matrix\n",
    "    \n",
    "    def backprop(self, x, y, lmda):\n",
    "        '''x = batch of input data, 2D matrix, examples x features\n",
    "           y = corresponding correct output values for the batch, 2D matrix, examples x outputs\n",
    "           lmda = regularization parameter\n",
    "           z = weighted input to neurons in layer l. \n",
    "           a = activation of neurons in layer l.\n",
    "           Layer 1 = input layer. No z value for layer 1, a_1 = x, i.e. no weights or activations\n",
    "           \n",
    "           Function returns the current cost for the batch and two lists of matrices:\n",
    "           nabla_w = list of partial derivatives of cost w.r.t. weights per layer\n",
    "           nabla_b = list of partial derivatives of cost w.r.t. biases per layer'''\n",
    "        num_data_points = x.shape[0]\n",
    "        z_vals = []\n",
    "        a_vals = [x]\n",
    "        \n",
    "        ''' Feedforward: storing all z and a values per layer '''\n",
    "        activation = Matrix(x)\n",
    "        for w, b in zip(self.weights[0:-1], self.biases[0:-1]):\n",
    "            z = activation * w + b\n",
    "            z_vals.append(z.matrix)\n",
    "            activation = Matrix(self.activationfnHidden.fn(z))\n",
    "            a_vals.append(activation.matrix)\n",
    "        z = activation * self.weights[-1] + self.biases[-1]\n",
    "        z_vals.append(z.matrix)\n",
    "        activation = Matrix(self.activationfnLast.fn(z))\n",
    "        a_vals.append(activation.matrix)\n",
    "              \n",
    "        cost = self.costfn.compute_cost(a_vals[-1], y, lmda, self.weights)\n",
    "        cost_prime = self.costfn.cost_prime(a_vals[-1], y)\n",
    "        \n",
    "        ''' Backprop: Errors per neuron calculated first starting at the last layer\n",
    "            and working backwards through the networks, then partial derivatives \n",
    "            are calculated for each set of weights and biases\n",
    "            deltas = neuron error per layer'''\n",
    "        deltas = []\n",
    "        \n",
    "        if (self.costfn.cost_type==\"QuadraticCost\"):\n",
    "            output_layer_delta = cost_prime * self.activationfnLast.prime(Matrix(z_vals[-1]))\n",
    "        elif (self.costfn.cost_type==\"CrossEntropyCost\" or self.costfn.cost_type==\"LogLikelihoodCost\"):\n",
    "            output_layer_delta = cost_prime\n",
    "        else:\n",
    "            print(\"No such cost function\")\n",
    "            exit(1)\n",
    "        \n",
    "        deltas.insert(0, output_layer_delta)\n",
    "        \n",
    "        for i in range(1,self.layers-1):\n",
    "            interim = np.dot(deltas[0], (np.transpose(self.weights[-i].matrix)))\n",
    "            act_prime = self.activationfnHidden.prime(Matrix(z_vals[-i-1]))\n",
    "            delta = np.multiply(interim, act_prime)\n",
    "            deltas.insert(0, delta)\n",
    "        \n",
    "        nabla_b = []\n",
    "        for i in range(len(deltas)):\n",
    "            interim = np.sum(deltas[i], axis=0) / num_data_points\n",
    "            nabla_b.append(np.reshape(interim, (1, interim.shape[0])))\n",
    "        \n",
    "        nabla_w = []\n",
    "        for i in range(0,self.layers-1):\n",
    "            interim = np.dot(np.transpose(a_vals[i]), deltas[i])\n",
    "            interim = interim / num_data_points\n",
    "            nabla_w.append(interim)\n",
    "        \n",
    "        return cost, nabla_b, nabla_w\n",
    "    \n",
    "    def update_weights(self, nabla_w, nabla_b, learning_rate, lmda, num_data_points):\n",
    "        '''nabla_w = list of partial derivatives of cost w.r.t. weights per layer\n",
    "           nabla_b = list of partial derivatives of cost w.r.t. biases per layer\n",
    "           learning_rate = learning rate hyperparamter, constrains size of parameter updates\n",
    "           lmda = regularization paramter\n",
    "           num_data_points = size of batch'''\n",
    "        \n",
    "        i = 0\n",
    "        weight_mult = 1 - ((learning_rate * lmda) / num_data_points)\n",
    "        for w, nw in zip(self.weights, nabla_w):\n",
    "            self.weights[i].matrix = weight_mult * w.matrix - learning_rate * nw\n",
    "            i += 1\n",
    "        i = 0\n",
    "        for b, nb in zip(self.biases, nabla_b):\n",
    "            self.biases[i].matrix = b.matrix - learning_rate * nb\n",
    "            i += 1\n",
    "            \n",
    "    def predict(self, x):\n",
    "        '''x = batch of input data, 2D matrix, examples x features\n",
    "           Function returns a 2D matrix of output values in one-hot encoded form'''\n",
    "        \n",
    "        output = self.feedforward(x)\n",
    "        if (output.shape[1]==1):\n",
    "            '''If only one output, convert to 1 if value > 0.5'''\n",
    "            low_indices = output <= 0.5\n",
    "            high_indices = output > 0.5\n",
    "            output[low_indices] = 0\n",
    "            output[high_indices] = 1\n",
    "        else:\n",
    "            '''Otherwise set maximum valued output element to 1, the rest to 0'''    \n",
    "            max_elem = output.max(axis=1)\n",
    "            max_elem = np.reshape(max_elem, (max_elem.shape[0], 1))\n",
    "            output = np.floor(output/ max_elem)\n",
    "        return output\n",
    "    \n",
    "    def accuracy(self, x, y):\n",
    "        '''x = batch of input data, 2D matrix, examples x features\n",
    "           y = corresponding correct output values for the batch, 2D matrix, examples x outputs\n",
    "           Function returns % of correct classified examples in the batch'''\n",
    "        prediction = self.predict(x)\n",
    "        num_data_points = x.shape[0]\n",
    "        if (prediction.shape[1]==1):\n",
    "            result = np.sum(prediction==y) / num_data_points\n",
    "        else:\n",
    "            result = np.sum(prediction.argmax(axis=1)==y.argmax(axis=1)) / num_data_points\n",
    "        return result\n",
    "    \n",
    "    def SGD(self, x, y, valid_x, valid_y, learning_rate, epochs, reporting_rate, lmda=0, batch_size=10, verbose=False):\n",
    "        '''Stochastic Gradient Descent\n",
    "           x = training data, 2D matrix, examples x features\n",
    "           y = corresponding correct output values for the training data, 2D matrix, examples x outputs\n",
    "           valid_x = validation data, 2D matrix, examples x features\n",
    "           valid_y = corresponding correct output values for the validation data, 2D matrix, examples x outputs\n",
    "           learning_rate = learning rate hyperparamter, constrains size of parameter updates\n",
    "           epochs = number of iterations through the entire training dataset\n",
    "           reporting_rate = rate at which to report information about the model's performance. \n",
    "           If the reporting rate is 10, then information will be printed every 10 epochs\n",
    "           lmda = regularization paramter\n",
    "           batch_size = batch size per parameter update. If batch size = 25 and there are 1000 examples in the \n",
    "           training data then there will be 40 updates per epoch\n",
    "           verbose: parameter controlling whether to print additional information. Useful for debugging\n",
    "           \n",
    "           Function returns two lists contraining the training and validation cost per parameter update\n",
    "           '''\n",
    "        training_cost = []\n",
    "        valid_cost = []\n",
    "        num_data_points = batch_size\n",
    "        total_data_points = x.shape[0]\n",
    "        output = self.feedforward(x)\n",
    "        cost = self.costfn.compute_cost(output,y,lmda,self.weights)\n",
    "        accuracy = self.accuracy(x, y)\n",
    "        valid_accuracy = self.accuracy(valid_x, valid_y)\n",
    "        print(\"Training cost at start of training is %.5f and accuracy is %3.2f%%\" % (cost, accuracy * 100))\n",
    "        print(\"Validation set accuracy is %3.2f%%\" % (valid_accuracy * 100))\n",
    "        if (verbose==True):\n",
    "            print(\"First 10 output values are:\")\n",
    "            print(output[0:10,:])\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            data = np.hstack((x,y))\n",
    "            input_dims = x.shape[1]\n",
    "            output_dims = y.shape[1]\n",
    "            np.random.shuffle(data)\n",
    "            batches = []\n",
    "            nabla_w =[]\n",
    "            nabla_b = []\n",
    "            \n",
    "            for k in range(0, (total_data_points - batch_size), batch_size):\n",
    "                batch = data[k:(k+batch_size),:]\n",
    "                batches.append(batch)\n",
    "            num_batches = len(batches)\n",
    "            for j in range(num_batches):\n",
    "                batch_x = batches[j][:,:input_dims]\n",
    "                batch_y = batches[j][:,input_dims:]\n",
    "                if (batch_y.ndim == 1):\n",
    "                    batch_y = np.reshape(batch_y, (batch_y.shape[0],1))\n",
    "                cost, nabla_b, nabla_w = self.backprop(batch_x, batch_y, lmda)\n",
    "                self.update_weights(nabla_w, nabla_b, learning_rate, lmda, num_data_points)\n",
    "\n",
    "                '''Monitoring progress (or lack of...)'''\n",
    "                training_cost.append(cost)\n",
    "                valid_output = self.feedforward(valid_x)\n",
    "                valid_c = self.costfn.compute_cost(valid_output,valid_y,lmda,self.weights)\n",
    "                valid_cost.append(valid_c)\n",
    "            \n",
    "            if (i % reporting_rate == 0):\n",
    "                output = self.feedforward(x)\n",
    "                cost = self.costfn.compute_cost(output,y,lmda,self.weights)\n",
    "                accuracy = self.accuracy(x, y)\n",
    "                valid_accuracy = self.accuracy(valid_x, valid_y)\n",
    "                print(\"Training cost in epoch %d is %.5f and accuracy is %3.2f%%\" % (i, cost, accuracy * 100))\n",
    "                print(\"Validation set accuracy is %3.2f%%\" % (valid_accuracy * 100))\n",
    "                if (verbose==True):\n",
    "                    print(\"First 10 output values are:\")\n",
    "                    print(output[0:10,:])\n",
    "                    print(\"Weight updates\")\n",
    "                    for i in range(len(self.weights)):\n",
    "                        print(nabla_w[i])\n",
    "                    print(\"Bias updates\")\n",
    "                    for i in range(len(self.biases)):\n",
    "                        print(nabla_b[i])\n",
    "                    print(\"Weights\")\n",
    "                    for i in range(len(self.weights)):\n",
    "                        print(self.weights[i].matrix)\n",
    "                    print(\"Biases\")\n",
    "                    for i in range(len(self.biases)):\n",
    "                        print(self.biases[i].matrix)\n",
    "                \n",
    "        '''Final results'''\n",
    "        output = self.feedforward(x)\n",
    "        cost = self.costfn.compute_cost(output,y,lmda,self.weights)\n",
    "        prediction = self.predict(x)\n",
    "        accuracy = self.accuracy(x, y)\n",
    "        valid_accuracy = self.accuracy(valid_x, valid_y)\n",
    "        print(\"Final test cost is %.5f\" % cost)\n",
    "        print(\"Accuracy on training data is %3.2f%%, and accuracy on validation data is %3.2f%%\" %\n",
    "              (accuracy * 100, valid_accuracy * 100))  \n",
    "        \n",
    "        return training_cost, valid_cost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
